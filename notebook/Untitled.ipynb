{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "infinite-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import dirname\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from utils.models import Classifier\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.loader import Loader\n",
    "from utils.loss import cross_entropy_loss_and_accuracy\n",
    "from utils.dataset import NCaltech101\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "academic-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(777)\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "resident-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Starting training with \n",
      "num_epochs: 2\n",
      "batch_size: 4\n",
      "device: cuda:0\n",
      "log_dir: /ws/external/log/temp\n",
      "training_dataset: /ws/data/N-Caltech101/training/\n",
      "validation_dataset: /ws/data/N-Caltech101/validation/\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "validation_dataset=\"/ws/data/N-Caltech101/validation/\"\n",
    "training_dataset=\"/ws/data/N-Caltech101/training/\"\n",
    "log_dir=\"/ws/external/log/temp\"\n",
    "device=\"cuda:0\"\n",
    "num_workers=4\n",
    "pin_memory=True\n",
    "batch_size=4\n",
    "num_epochs=2\n",
    "save_every_n_epochs=2\n",
    "checkpoint = \"/ws/external/exp1/model_best.pth\" # model_best.pth checkpoint_13625_0.5990.pth\n",
    "    \n",
    "    \n",
    "assert os.path.isdir(dirname(log_dir)), f\"Log directory root {dirname(log_dir)} not found.\"\n",
    "assert os.path.isdir(validation_dataset), f\"Validation dataset directory {validation_dataset} not found.\"\n",
    "assert os.path.isdir(training_dataset), f\"Training dataset directory {training_dataset} not found.\"\n",
    "\n",
    "print(f\"----------------------------\\n\"\n",
    "      f\"Starting training with \\n\"\n",
    "      f\"num_epochs: {num_epochs}\\n\"\n",
    "      f\"batch_size: {batch_size}\\n\"\n",
    "      f\"device: {device}\\n\"\n",
    "      f\"log_dir: {log_dir}\\n\"\n",
    "      f\"training_dataset: {training_dataset}\\n\"\n",
    "      f\"validation_dataset: {validation_dataset}\\n\"\n",
    "      f\"----------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "executed-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(t, q):\n",
    "    B, C, H, W = t.shape\n",
    "    k = 1 + round(.01 * float(q) * (C * H * W - 1))\n",
    "    result = t.view(B, -1).kthvalue(k).values\n",
    "    return result[:,None,None,None]\n",
    "\n",
    "def create_image(representation):\n",
    "    B, C, H, W = representation.shape\n",
    "    representation = representation.view(B, 3, C // 3, H, W).sum(2)\n",
    "\n",
    "    # do robust min max norm\n",
    "    representation = representation.detach().cpu()\n",
    "    robust_max_vals = percentile(representation, 99)\n",
    "    robust_min_vals = percentile(representation, 1)\n",
    "\n",
    "    representation = (representation - robust_min_vals)/(robust_max_vals - robust_min_vals)\n",
    "    representation = torch.clamp(255*representation, 0, 255).byte()\n",
    "\n",
    "    representation = torchvision.utils.make_grid(representation)\n",
    "\n",
    "    return representation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "regulation-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, dataset, batch_size=2, num_workers=2, pin_memory=True, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        split_indices = list(range(len(dataset)))\n",
    "        sampler = torch.utils.data.sampler.SubsetRandomSampler(split_indices)\n",
    "        self.loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                                             num_workers=num_workers, pin_memory=pin_memory,\n",
    "                                             collate_fn=collate_events)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.loader:\n",
    "            data = [d.to(self.device) for d in data]\n",
    "            yield data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "def collate_events(data):\n",
    "    labels = []\n",
    "    events = []\n",
    "    for i, d in enumerate(data):\n",
    "        labels.append(d[1])\n",
    "        ev = np.concatenate([d[0], i*np.ones((len(d[0]),1), dtype=np.float32)],1)\n",
    "        events.append(ev)\n",
    "    events = torch.from_numpy(np.concatenate(events,0))\n",
    "    labels = default_collate(labels)\n",
    "    return events, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "revised-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets, add augmentation to training set\n",
    "training_dataset = NCaltech101(training_dataset, augmentation=True)\n",
    "validation_dataset = NCaltech101(validation_dataset)\n",
    "\n",
    "# construct loader, handles data streaming to gpu\n",
    "training_loader = Loader(training_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, device=\"cuda:0\")\n",
    "validation_loader = Loader(validation_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "timely-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, and put to device\n",
    "model = Classifier(pretrained=False)\n",
    "ckpt = torch.load(checkpoint)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model = model.to(device)\n",
    "\n",
    "# # optimizer and lr scheduler\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "iteration = 0\n",
    "min_validation_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "complete-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (events, labels) = next(enumerate(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eight-limitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([624687, 5])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "indie-iceland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24, 32, 73, 38], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "resistant-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "pred_labels, representation = model(events)\n",
    "# loss, accuracy = cross_entropy_loss_and_accuracy(pred_labels, labels)\n",
    "# loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "satisfactory-chicken",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 33,  76,  92, 100], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "determined-leader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 101])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "precise-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_vizualization = create_image(representation)\n",
    "writer.add_image(\"training/representation\", representation_vizualization, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-dividend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-drunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-anthony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-scott",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-double",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-torture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "delayed-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train classifier using a learnt quantization layer. [-h]\n",
      "                                                           --validation_dataset\n",
      "                                                           VALIDATION_DATASET\n",
      "                                                           --training_dataset\n",
      "                                                           TRAINING_DATASET\n",
      "                                                           --log_dir LOG_DIR\n",
      "                                                           [--device DEVICE]\n",
      "                                                           [--num_workers NUM_WORKERS]\n",
      "                                                           [--pin_memory PIN_MEMORY]\n",
      "                                                           [--batch_size BATCH_SIZE]\n",
      "                                                           [--num_epochs NUM_EPOCHS]\n",
      "                                                           [--save_every_n_epochs SAVE_EVERY_N_EPOCHS]\n",
      "Train classifier using a learnt quantization layer.: error: the following arguments are required: --validation_dataset, --training_dataset, --log_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(num_epochs):\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    model = model.eval()\n",
    "\n",
    "    print(f\"Validation step [{i:3d}/{num_epochs:3d}]\")\n",
    "    for events, labels in tqdm.tqdm(validation_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_labels, representation = model(events)\n",
    "            loss, accuracy = cross_entropy_loss_and_accuracy(pred_labels, labels)\n",
    "\n",
    "        sum_accuracy += accuracy\n",
    "        sum_loss += loss\n",
    "\n",
    "    validation_loss = sum_loss.item() / len(validation_loader)\n",
    "    validation_accuracy = sum_accuracy.item() / len(validation_loader)\n",
    "\n",
    "    writer.add_scalar(\"validation/accuracy\", validation_accuracy, iteration)\n",
    "    writer.add_scalar(\"validation/loss\", validation_loss, iteration)\n",
    "\n",
    "    # visualize representation\n",
    "    representation_vizualization = create_image(representation)\n",
    "    writer.add_image(\"validation/representation\", representation_vizualization, iteration)\n",
    "\n",
    "    print(f\"Validation Loss {validation_loss:.4f}  Accuracy {validation_accuracy:.4f}\")\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        torch.save({\n",
    "            \"state_dict\": state_dict,\n",
    "            \"min_val_loss\": min_validation_loss,\n",
    "            \"iteration\": iteration\n",
    "        }, \"log/model_best.pth\")\n",
    "        print(\"New best at \", validation_loss)\n",
    "\n",
    "    if i % save_every_n_epochs == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save({\n",
    "            \"state_dict\": state_dict,\n",
    "            \"min_val_loss\": min_validation_loss,\n",
    "            \"iteration\": iteration\n",
    "        }, \"log/checkpoint_%05d_%.4f.pth\" % (iteration, min_validation_loss))\n",
    "\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "\n",
    "    model = model.train()\n",
    "    print(f\"Training step [{i:3d}/{num_epochs:3d}]\")\n",
    "    for events, labels in tqdm.tqdm(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_labels, representation = model(events)\n",
    "        loss, accuracy = cross_entropy_loss_and_accuracy(pred_labels, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_accuracy += accuracy\n",
    "        sum_loss += loss\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    if i % 10 == 9:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    training_loss = sum_loss.item() / len(training_loader)\n",
    "    training_accuracy = sum_accuracy.item() / len(training_loader)\n",
    "    print(f\"Training Iteration {iteration:5d}  Loss {training_loss:.4f}  Accuracy {training_accuracy:.4f}\")\n",
    "\n",
    "    writer.add_scalar(\"training/accuracy\", training_accuracy, iteration)\n",
    "    writer.add_scalar(\"training/loss\", training_loss, iteration)\n",
    "\n",
    "    representation_vizualization = create_image(representation)\n",
    "    writer.add_image(\"training/representation\", representation_vizualization, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
